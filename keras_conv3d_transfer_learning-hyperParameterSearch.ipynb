{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljames/py-3.5/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "# Third-Party Imports\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from IPython.display import display\n",
    "from JSAnimation import IPython_display\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Local Imports\n",
    "import c3d_model\n",
    "import clip_dataset\n",
    "from clip_dataset import DataGenerator\n",
    "import config_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    " \n",
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] image_dim_order (from default ~/.keras/keras.json)=tf\n"
     ]
    }
   ],
   "source": [
    "# Use tf backend\n",
    "dim_ordering = K.image_dim_ordering()\n",
    "print(\"[Info] image_dim_order (from default ~/.keras/keras.json)={}\".format(\n",
    "        dim_ordering))\n",
    "backend = dim_ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(vid, size):\n",
    "    html_code = \"\"\"\n",
    "    <video width=\"{}\" height=\"{}\" controls>\n",
    "      <source src={} type=\"video/mp4\">\n",
    "    </video>\"\"\".format(size[0], size[1], vid)\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitions(PATH):\n",
    "    \"\"\"\n",
    "    Return dictionary that places each filename into\n",
    "    a list with the parent dataset (train/valid/test) as the key\n",
    "    \"\"\"\n",
    "    datasets = listdir(PATH)\n",
    "    # print('datasets are: {}'.format(datasets))\n",
    "    partitions = {d:[] for d in datasets}\n",
    "\n",
    "    for d in datasets:\n",
    "        classes = listdir(join(PATH, d))\n",
    "        for c in classes:\n",
    "            files = listdir(join(PATH, d, c))\n",
    "            [partitions[d].append(join(PATH, d, c, f)) for f in files]\n",
    "        # Randomize order\n",
    "        shuffle(partitions[d])\n",
    "\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(model_dir, metric='acc'):\n",
    "    \"\"\"\n",
    "    Return path to model weights with either lowest\n",
    "    loss or highest accuracy\n",
    "    \"\"\"\n",
    "    # Get all paths\n",
    "    paths = listdir(model_dir)\n",
    "    \n",
    "    # Get only weight files\n",
    "    weights = [p for p in paths if p[-5:] == '.hdf5']\n",
    "    \n",
    "    # Get only type of weights that were saved by desired metric\n",
    "    weights = [w for w in weights if metric in w]\n",
    "\n",
    "        \n",
    "    vals = [float(w.rsplit('.hdf5', 1)[0].rsplit('-', 1)[-1]) for w in weights]\n",
    "    if metric == 'acc':\n",
    "        best_val = max(vals)\n",
    "    else:\n",
    "        best_val = min(vals)\n",
    "        \n",
    "    best_model = weights[vals.index(best_val)]\n",
    "    return join(model_dir, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(PATH, classes_to_nums):\n",
    "    \"\"\"\n",
    "    Return dictionary that places each filename into\n",
    "    a list with the parent dataset as the key\n",
    "    \"\"\"\n",
    "    datasets = listdir(PATH)\n",
    "    print('datasets are: {}'.format(datasets))\n",
    "    labels = {}\n",
    "\n",
    "    for d in datasets:\n",
    "        classes = listdir(join(PATH, d))\n",
    "        for c in classes:\n",
    "            files = listdir(join(PATH, d, c))\n",
    "            num = classes_to_nums[c]\n",
    "            temp = {join(PATH, d, c, f):num for f in files}\n",
    "            labels = {**temp, **labels}\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(dense_activation='relu'):\n",
    "    show_images = False\n",
    "    diagnose_plots = False\n",
    "    pretrained_model_dir = './models'\n",
    "    global backend\n",
    "\n",
    "    print(\"[Info] Using backend={}\".format(backend))\n",
    "\n",
    "    if backend == 'th':\n",
    "        model_weight_filename = join(pretrained_model_dir, 'sports1M_weights_th.h5')\n",
    "        model_json_filename = join(pretrained_model_dir, 'sports1M_weights_th.json')\n",
    "    else:\n",
    "        model_weight_filename = join(pretrained_model_dir, 'sports1M_weights_tf.h5')\n",
    "        model_json_filename = join(pretrained_model_dir, 'sports1M_weights_tf.json')\n",
    "\n",
    "    print(\"[Info] Reading model architecture...\")\n",
    "    model_pretrained = model_from_json(open(model_json_filename, 'r').read())\n",
    "    # print(model_pretrained.summary())\n",
    "\n",
    "    # visualize model\n",
    "    \"\"\"\n",
    "    model_img_filename = os.path.join(pretrained_model_dir, 'c3d_model.png')\n",
    "    if not os.path.exists(model_img_filename):\n",
    "        from keras.utils import plot_model\n",
    "        plot_model(model, to_file=model_img_filename)\n",
    "    \"\"\"\n",
    "    # Load pretrained weights\n",
    "    print(\"[Info] Loading model weights...\")\n",
    "    model_pretrained.load_weights(model_weight_filename)\n",
    "    print(\"[Info] Loading model weights -- DONE!\")\n",
    "    model_pretrained.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "    # Change output layer\n",
    "    # model_pretrained.layers.pop()\n",
    "    # new_out = Dense(2, activation='softmax', name='fc9')(model_pretrained.layers[-1].output)\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    x = model_pretrained.layers[-1].output\n",
    "    x = Dense(128, activation=dense_activation, name='fc7')(x)\n",
    "    x = Dropout(0.5, name='dropout_1')(x)\n",
    "    x = Dense(128, activation=dense_activation, name='fc8')(x)\n",
    "    x = Dropout(0.5, name='dropout_2')(x)\n",
    "    x = Dense(2, activation='softmax', name='fc9')(x)\n",
    "    # x = Dense(2, activation='softmax', name='fc9')\n",
    "    # new_out = (d1)(d2)(d3)\n",
    "    # model = Model(model_pretrained.input, output=[x])\n",
    "    return Model(model_pretrained.input, output=[x])\n",
    "    # model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save_inference_results(model, dataset_generator, path, trials=3):\n",
    "    inference_results = []\n",
    "    for i in range(trials):\n",
    "        single_inference = model.evaluate_generator(generator=dataset_generator)\n",
    "        inference_results.append(single_inference)\n",
    "    pickle.dump(inference_results, open(path, \"wb\" ))\n",
    "    return inference_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator):\n",
    "    \n",
    "    # Run inference on model as is, model with the best validation accuracy, and model with the best validation loss\n",
    "    metric = 'final'\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)\n",
    "\n",
    "    metric = 'acc'\n",
    "    best_model = get_best_model(model_dir, metric=metric)\n",
    "    model.load_weights(best_model)\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)\n",
    "\n",
    "    metric = 'loss'\n",
    "    best_model = get_best_model(model_dir, metric=metric)\n",
    "    model.load_weights(best_model)\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of special over-the-weekend run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = config_clips.dataset_dir\n",
    "classes_to_nums = config_clips.classes_to_nums\n",
    "train_params = config_clips.train_params\n",
    "valid_params = config_clips.valid_params\n",
    "test_params = config_clips.test_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets are: ['test', 'valid', 'train']\n"
     ]
    }
   ],
   "source": [
    "# Generators\n",
    "partition = get_partitions(PATH)\n",
    "labels = get_labels(PATH, classes_to_nums)\n",
    "training_generator = DataGenerator(partition['train'], labels, **train_params)\n",
    "validation_generator = DataGenerator(partition['valid'], labels, **valid_params)\n",
    "testing_generator = DataGenerator(partition['test'], labels, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljames/py-3.5/lib/python3.5/site-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(Tensor(\"co..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "20/20 [==============================] - 30s 1s/step - loss: 3.9400 - acc: 0.5469 - val_loss: 1.2097 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68750, saving model to ../models/model_c3d_030/weights-acc-improvement-001-0.6875.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20974, saving model to ../models/model_c3d_030/weights-loss-improvement-001-1.2097.hdf5\n",
      "Slept - allocating memory for next model\n",
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.7889 - acc: 0.5437 - val_loss: 0.6234 - val_acc: 0.6635\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66346, saving model to ../models/model_c3d_031/weights-acc-improvement-001-0.6635.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62338, saving model to ../models/model_c3d_031/weights-loss-improvement-001-0.6234.hdf5\n",
      "Slept - allocating memory for next model\n",
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.8160 - acc: 0.5062 - val_loss: 0.6025 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79327, saving model to ../models/model_c3d_032/weights-acc-improvement-001-0.7933.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60252, saving model to ../models/model_c3d_032/weights-loss-improvement-001-0.6025.hdf5\n",
      "Slept - allocating memory for next model\n",
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.7664 - acc: 0.5594 - val_loss: 0.5774 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76442, saving model to ../models/model_c3d_033/weights-acc-improvement-001-0.7644.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57736, saving model to ../models/model_c3d_033/weights-loss-improvement-001-0.5774.hdf5\n",
      "Slept - allocating memory for next model\n",
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n",
      "Epoch 1/1\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.7406 - acc: 0.5844 - val_loss: 0.5685 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78365, saving model to ../models/model_c3d_034/weights-acc-improvement-001-0.7837.hdf5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56854, saving model to ../models/model_c3d_034/weights-loss-improvement-001-0.5685.hdf5\n",
      "Slept - allocating memory for next model\n",
      "[Info] Using backend=tf\n",
      "[Info] Reading model architecture...\n",
      "[Info] Loading model weights...\n",
      "[Info] Loading model weights -- DONE!\n",
      "Epoch 1/1\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.7085 - acc: 0.6217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-97:\n",
      "Process ForkPoolWorker-100:\n",
      "Process ForkPoolWorker-101:\n",
      "Process ForkPoolWorker-94:\n",
      "Process ForkPoolWorker-98:\n",
      "Process ForkPoolWorker-91:\n",
      "Process ForkPoolWorker-95:\n",
      "Process ForkPoolWorker-102:\n",
      "Process ForkPoolWorker-96:\n",
      "Process ForkPoolWorker-99:\n",
      "Process ForkPoolWorker-93:\n",
      "Process ForkPoolWorker-92:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f89b72d8a76b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         workers=6)\n\u001b[0m\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Saves final model and training results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m   2245\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2378\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(4, 16):\n",
    "    # loads unique names for specific training session\n",
    "    model_iteration = 'model_c3d_0' + str(26 + i)\n",
    "    model_dir = join(\"..\", \"models\", model_iteration)\n",
    "    model_name = join(model_dir, model_iteration + '.h5')\n",
    "    history_name = join(model_dir, model_iteration + '_history.pkl')\n",
    "\n",
    "    # makes new directory to place all saved files\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    # Callbacks\n",
    "    filepath = join(model_dir, \"weights-acc-improvement-{epoch:03d}-{val_acc:.4f}.hdf5\")\n",
    "    checkpoint_acc = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    filepath = join(model_dir, \"weights-loss-improvement-{epoch:03d}-{val_loss:.4f}.hdf5\")\n",
    "    checkpoint_loss = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint_acc, checkpoint_loss]\n",
    "\n",
    "    # picks an activation\n",
    "    if i < 5:\n",
    "        activation = 'relu'\n",
    "    elif i < 10:\n",
    "        activation = 'sigmoid'\n",
    "    elif i < 15:\n",
    "        activation = 'softmax'\n",
    "    else:\n",
    "        activation = 'relu'\n",
    "    \n",
    "    # loads a model\n",
    "    model = load_model(dense_activation=activation)\n",
    "    layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layers_to_train:\n",
    "            layer.trainable = True\n",
    "            # print('{} IS trainable'.format(layer.name))\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "    # compiles a model \n",
    "    adam = optimizers.adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    # trains a model\n",
    "    history = model.fit_generator(\n",
    "                        generator=training_generator,\n",
    "                        steps_per_epoch=20,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        epochs=1,\n",
    "                        initial_epoch=0,\n",
    "                        workers=6)\n",
    "    \"\"\"\n",
    "    # Saves final model and training results\n",
    "    # print('Saving model as {}'.format(model_name))\n",
    "    model.save(model_name)\n",
    "    with open(history_name, \"wb\" ) as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    # Runs inference verbosely over datasets\n",
    "    run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)\"\"\"\n",
    "    \n",
    "    del history\n",
    "    del model\n",
    "    K.clear_session()\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    print(\"Slept - allocating memory for next model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6, 10):\n",
    "    # loads unique names for specific training session\n",
    "    model_iteration = 'model_c3d_0' + str(41 + i)\n",
    "    model_dir = join(\"..\", \"models\", model_iteration)\n",
    "    model_name = join(model_dir, model_iteration + '.h5')\n",
    "    history_name1 = join(model_dir, model_iteration + '_history1.pkl')\n",
    "    history_name2 = join(model_dir, model_iteration + '_history2.pkl')\n",
    "    weights_name = join(model_dir, model_iteration + '.hdf5')\n",
    "\n",
    "    # makes new directory to place all saved files\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Callbacks\n",
    "    filepath = join(model_dir, \"weights-acc-improvement-{epoch:03d}-{val_acc:.4f}.hdf5\")\n",
    "    checkpoint_acc = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    filepath = join(model_dir, \"weights-loss-improvement-{epoch:03d}-{val_loss:.4f}.hdf5\")\n",
    "    checkpoint_loss = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint_acc, checkpoint_loss]\n",
    "    \n",
    "    \n",
    "    # INTIAL training\n",
    "    \n",
    "    # loads a model\n",
    "    model = load_model(dense_activation='softmax')\n",
    "    layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layers_to_train:\n",
    "            layer.trainable = True\n",
    "            # print('{} IS trainable'.format(layer.name))\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "    # compiles a model \n",
    "    adam = optimizers.adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    # trains a model\n",
    "    history = model.fit_generator(\n",
    "                        generator=training_generator,\n",
    "                        steps_per_epoch=20,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        epochs=300,\n",
    "                        initial_epoch=0,\n",
    "                        workers=6)\n",
    "        \n",
    "    # model.save(model_name)\n",
    "    model.save_weights(weights_name)\n",
    "    with open(history_name1, \"wb\" ) as f1:\n",
    "        pickle.dump(history.history, f1)\n",
    "\n",
    "    # RETRAINING\n",
    "    # picks an activation\n",
    "    if i < 5:\n",
    "        activation = 'relu'\n",
    "    elif i < 10:\n",
    "        activation = 'sigmoid'\n",
    "    else:\n",
    "        activation = 'relu'\n",
    "    \n",
    "    # loads a model\n",
    "    model = load_model(dense_activation=activation)\n",
    "    layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layers_to_train:\n",
    "            layer.trainable = True\n",
    "            # print('{} IS trainable'.format(layer.name))\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "    # compiles a model \n",
    "    adam = optimizers.adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.load_weights(weights_name)\n",
    "    # trains a model\n",
    "    history = model.fit_generator(\n",
    "                        generator=training_generator,\n",
    "                        steps_per_epoch=20,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        epochs=400,\n",
    "                        initial_epoch=300,\n",
    "                        workers=6)\n",
    "\n",
    "    # Saves final model and training results\n",
    "    # print('Saving model as {}'.format(model_name))\n",
    "    model.save(model_name)\n",
    "    with open(history_name2, \"wb\" ) as f2:\n",
    "        pickle.dump(history.history, f2)\n",
    "\n",
    "    # Runs inference verbosely over datasets\n",
    "    run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of special over-the-weekend run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
