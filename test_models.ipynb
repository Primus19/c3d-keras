{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "\n",
    "# Third-Party Imports\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from IPython.display import display\n",
    "from JSAnimation import IPython_display\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Local Imports\n",
    "import c3d_model\n",
    "import clip_dataset\n",
    "from clip_dataset import DataGenerator\n",
    "import config_clips\n",
    "\n",
    "# Use tf backend\n",
    "dim_ordering = K.image_dim_ordering()\n",
    "print(\"[Info] image_dim_order (from default ~/.keras/keras.json)={}\".format(\n",
    "        dim_ordering))\n",
    "backend = dim_ordering\n",
    "\n",
    "def play_video(vid, size):\n",
    "    html_code = \"\"\"\n",
    "    <video width=\"{}\" height=\"{}\" controls>\n",
    "      <source src={} type=\"video/mp4\">\n",
    "    </video>\"\"\".format(size[0], size[1], vid)\n",
    "    return html_code\n",
    "\n",
    "def get_partitions(PATH):\n",
    "    \"\"\"\n",
    "    Return dictionary that places each filename into\n",
    "    a list with the parent dataset (train/valid/test) as the key\n",
    "    \"\"\"\n",
    "    datasets = listdir(PATH)\n",
    "    # print('datasets are: {}'.format(datasets))\n",
    "    partitions = {d:[] for d in datasets}\n",
    "\n",
    "    for d in datasets:\n",
    "        classes = listdir(join(PATH, d))\n",
    "        for c in classes:\n",
    "            files = listdir(join(PATH, d, c))\n",
    "            [partitions[d].append(join(PATH, d, c, f)) for f in files]\n",
    "        # Randomize order\n",
    "        shuffle(partitions[d])\n",
    "\n",
    "    return partitions\n",
    "\n",
    "def get_best_model(model_dir, metric='acc'):\n",
    "    \"\"\"\n",
    "    Return path to model weights with either lowest\n",
    "    loss or highest accuracy\n",
    "    \"\"\"\n",
    "    # Get all paths\n",
    "    paths = listdir(model_dir)\n",
    "    \n",
    "    # Get only weight files\n",
    "    weights = [p for p in paths if p[-5:] == '.hdf5']\n",
    "    \n",
    "    # Get only type of weights that were saved by desired metric\n",
    "    weights = [w for w in weights if metric in w]\n",
    "\n",
    "        \n",
    "    vals = [float(w.rsplit('.hdf5', 1)[0].rsplit('-', 1)[-1]) for w in weights]\n",
    "    if metric == 'acc':\n",
    "        best_val = max(vals)\n",
    "    else:\n",
    "        best_val = min(vals)\n",
    "        \n",
    "    best_model = weights[vals.index(best_val)]\n",
    "    return join(model_dir, best_model)\n",
    "\n",
    "def get_labels(PATH, classes_to_nums):\n",
    "    \"\"\"\n",
    "    Return dictionary that places each filename into\n",
    "    a list with the parent dataset as the key\n",
    "    \"\"\"\n",
    "    datasets = listdir(PATH)\n",
    "    print('datasets are: {}'.format(datasets))\n",
    "    labels = {}\n",
    "\n",
    "    for d in datasets:\n",
    "        classes = listdir(join(PATH, d))\n",
    "        for c in classes:\n",
    "            files = listdir(join(PATH, d, c))\n",
    "            num = classes_to_nums[c]\n",
    "            temp = {join(PATH, d, c, f):num for f in files}\n",
    "            labels = {**temp, **labels}\n",
    "\n",
    "    return labels\n",
    "\n",
    "def make_predictions(model, data_gen):\n",
    "    \"\"\"Gets a batch from a data_gen and gets predictions on it\"\"\"\n",
    "    batch = randint(0, data_gen.__len__())\n",
    "    data_x, data_y = data_gen.__getitem__(batch)\n",
    "    prediction_results = model.predict_on_batch(data_x)\n",
    "    \n",
    "    return data_x, data_y, prediction_results\n",
    "\n",
    "def plot_predictions(r, c, data_x, data_y, probs, classes):\n",
    "    \"\"\"\n",
    "    Creates a grid of plots of predictions and goundtruth labels of \n",
    "    input images\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(r, c)\n",
    "    fig.set_figheight(r * 6)\n",
    "    fig.set_figwidth(c * 10)\n",
    "    for i in range(r*c):\n",
    "\n",
    "        # Read image into RGB format\n",
    "        img = np.asarray(data_x[i, 0, :, :, :])\n",
    "        # img = np.reshape(img, [1280, 720, 3])\n",
    "        \n",
    "        mean_cube = np.load('models/train01_16_128_171_mean.npy')\n",
    "        mean_cube = np.transpose(mean_cube, (1, 2, 3, 0))\n",
    "        mean_cube = mean_cube[:, 8:120, 30:142, :]\n",
    "        # img += mean_cube[0, :, :, :]\n",
    "\n",
    "        # get predicted and groudtruth labels\n",
    "        lbl = data_y[i][0]\n",
    "        pred = probs[i]\n",
    "        pred = [round(z*1000)/1000 for z in pred]\n",
    "        prb = probs[i][0]\n",
    "        if prb > 0.5:\n",
    "            cls = classes[0]\n",
    "        else:\n",
    "            cls = classes[1]\n",
    "\n",
    "        if lbl > 0.5:\n",
    "            lbl = classes[0]\n",
    "        else:\n",
    "            lbl = classes[1]\n",
    "        \n",
    "        # Add to subplot\n",
    "        row = i % r\n",
    "        col = i // r\n",
    "        axes[row, col].imshow((img + mean_cube[0, :, :, :]) / 256)\n",
    "        axes[row, col].set_title('out:{}, pred:{}, truth:{}'.format(\n",
    "            pred, cls, lbl))\n",
    "    plt.show()\n",
    "\n",
    "def plot_movie_array_js(r, c, data_x, data_y, probs, classes):\n",
    "    \"\"\"\n",
    "    Plots an array of movies with their predictions and\n",
    "    groud truth labels as the titles\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(r, c)\n",
    "    fig.set_figheight(r * 10)\n",
    "    fig.set_figwidth(c * 10)\n",
    "    mean_cube = np.load('models/train01_16_128_171_mean.npy')\n",
    "    mean_cube = np.transpose(mean_cube, (1, 2, 3, 0))\n",
    "    mean_cube = mean_cube[:, 8:120, 30:142, :]\n",
    "    # img += mean_cube[0, :, :, :]   \n",
    "\n",
    "    def animate(z):\n",
    "        for i in range(r*c):\n",
    "\n",
    "            # get predicted and groudtruth labels\n",
    "            lbl = data_y[i][0]\n",
    "            pred = probs[i]\n",
    "            pred = [round(z*1000)/1000 for z in pred]\n",
    "            prb = probs[i][0]\n",
    "            if prb > 0.5:\n",
    "                cls = classes[0]\n",
    "            else:\n",
    "                cls = classes[1]\n",
    "\n",
    "            if lbl > 0.5:\n",
    "                lbl = classes[0]\n",
    "            else:\n",
    "                lbl = classes[1]\n",
    "\n",
    "            # Add to subplot\n",
    "            row = i % r\n",
    "            col = i // r\n",
    "            img = (data_x[i, :, :, :, :] + mean_cube) / 256 \n",
    "            axes[row, col].imshow(img[z]).set_array(img[z])\n",
    "            axes[row, col].set_title(\n",
    "                        'out:{}, pred:{}, truth:{}'.format(pred, cls, lbl), \n",
    "                        fontsize=20)\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=16)\n",
    "    display(IPython_display.display_animation(anim))\n",
    "\n",
    "def plot_movie_js(image_array):\n",
    "    dpi = 72.0\n",
    "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
    "    im = plt.figimage(image_array[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(image_array[i])\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
    "    display(IPython_display.display_animation(anim))\n",
    "\n",
    "def predictions_over_entire_gen(model, data_gen):\n",
    "    \"\"\"Gets a batch from a data_gen and gets predictions on it\"\"\"\n",
    "    predictions = []\n",
    "    truth = []\n",
    "    # diffs = []\n",
    "    imgs = []\n",
    "    for i in range(0, data_gen.__len__()):\n",
    "        data_x, data_y = data_gen.__getitem__(i)\n",
    "        # print(int(round(model.predict_on_batch(data_x)[0][1])))\n",
    "        truth.append(int(round(data_y[0][1])))\n",
    "        predictions.append(model.predict_on_batch(data_x)[0][1])\n",
    "        imgs.append(i)\n",
    "\n",
    "    return truth, predictions, imgs\n",
    "\n",
    "def predict_class_over_entire_gen(model, data_gen):\n",
    "    \"\"\"Gets a batch from a data_gen and gets predictions on it\"\"\"\n",
    "    predictions = []\n",
    "    truth = []\n",
    "    for i in range(0, data_gen.__len__()):\n",
    "        data_x, data_y = data_gen.__getitem__(i)\n",
    "        # print(int(round(model.predict_on_batch(data_x)[0][1])))\n",
    "        truth.append(int(round(data_y[0][1])))\n",
    "        predictions.append(int(round(model.predict_on_batch(data_x)[0][1])))\n",
    "    return truth, predictions\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def load_model(dense_activation='relu'):\n",
    "    show_images = False\n",
    "    diagnose_plots = False\n",
    "    pretrained_model_dir = './models'\n",
    "    global backend\n",
    "\n",
    "    print(\"[Info] Using backend={}\".format(backend))\n",
    "\n",
    "    if backend == 'th':\n",
    "        model_weight_filename = join(pretrained_model_dir, 'sports1M_weights_th.h5')\n",
    "        model_json_filename = join(pretrained_model_dir, 'sports1M_weights_th.json')\n",
    "    else:\n",
    "        model_weight_filename = join(pretrained_model_dir, 'sports1M_weights_tf.h5')\n",
    "        model_json_filename = join(pretrained_model_dir, 'sports1M_weights_tf.json')\n",
    "\n",
    "    print(\"[Info] Reading model architecture...\")\n",
    "    model_pretrained = model_from_json(open(model_json_filename, 'r').read())\n",
    "    # print(model_pretrained.summary())\n",
    "\n",
    "    # visualize model\n",
    "    \"\"\"\n",
    "    model_img_filename = os.path.join(pretrained_model_dir, 'c3d_model.png')\n",
    "    if not os.path.exists(model_img_filename):\n",
    "        from keras.utils import plot_model\n",
    "        plot_model(model, to_file=model_img_filename)\n",
    "    \"\"\"\n",
    "    # Load pretrained weights\n",
    "    print(\"[Info] Loading model weights...\")\n",
    "    model_pretrained.load_weights(model_weight_filename)\n",
    "    print(\"[Info] Loading model weights -- DONE!\")\n",
    "    model_pretrained.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "    # Change output layer\n",
    "    # model_pretrained.layers.pop()\n",
    "    # new_out = Dense(2, activation='softmax', name='fc9')(model_pretrained.layers[-1].output)\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    model_pretrained.layers.pop()\n",
    "    x = model_pretrained.layers[-1].output\n",
    "    x = Dense(128, activation=dense_activation, name='fc7')(x)\n",
    "    x = Dropout(0.5, name='dropout_1')(x)\n",
    "    x = Dense(128, activation=dense_activation, name='fc8')(x)\n",
    "    x = Dropout(0.5, name='dropout_2')(x)\n",
    "    x = Dense(2, activation='softmax', name='fc9')(x)\n",
    "    # x = Dense(2, activation='softmax', name='fc9')\n",
    "    # new_out = (d1)(d2)(d3)\n",
    "    # model = Model(model_pretrained.input, output=[x])\n",
    "    return Model(model_pretrained.input, output=[x])\n",
    "    # model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "def run_and_save_inference_results(model, dataset_generator, path, trials=3):\n",
    "    inference_results = []\n",
    "    for i in range(trials):\n",
    "        single_inference = model.evaluate_generator(generator=dataset_generator)\n",
    "        inference_results.append(single_inference)\n",
    "    pickle.dump(inference_results, open(path, \"wb\" ))\n",
    "    return inference_results\n",
    "\n",
    "def run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator):\n",
    "    \n",
    "    # Run inference on model as is, model with the best validation accuracy, and model with the best validation loss\n",
    "    metric = 'final'\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)\n",
    "\n",
    "    metric = 'acc'\n",
    "    best_model = get_best_model(model_dir, metric=metric)\n",
    "    model.load_weights(best_model)\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)\n",
    "\n",
    "    metric = 'loss'\n",
    "    best_model = get_best_model(model_dir, metric=metric)\n",
    "    model.load_weights(best_model)\n",
    "    run_and_save_inference_results(model, training_generator, join(model_dir, metric + '_training_results.pkl'), trials=1)\n",
    "    run_and_save_inference_results(model, validation_generator, join(model_dir, metric + '_validation_results.pkl'), trials=5)\n",
    "    run_and_save_inference_results(model, testing_generator, join(model_dir, metric + '_testing_results.pkl'), trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_trn = config_clips.train_data_dir\n",
    "PATH_val = config_clips.valid_data_dir\n",
    "PATH_test =  config_clips.test_data_dir\n",
    "PATH = config_clips.dataset_dir\n",
    "sizes = config_clips.sizes\n",
    "classes_to_nums = config_clips.classes_to_nums\n",
    "train_params = config_clips.train_params\n",
    "valid_params = config_clips.valid_params\n",
    "test_params = config_clips.test_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = join(\n",
    "            model_dir, \n",
    "            \"weights-acc-improvement-{epoch:03d}-{val_acc:.4f}.hdf5\")\n",
    "\n",
    "checkpoint_acc = ModelCheckpoint(\n",
    "                            filepath, \n",
    "                            monitor='val_acc', \n",
    "                            verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            mode='max')\n",
    "\n",
    "filepath = join(\n",
    "            model_dir, \n",
    "            \"weights-loss-improvement-{epoch:03d}-{val_loss:.4f}.hdf5\")\n",
    "\n",
    "checkpoint_loss = ModelCheckpoint(\n",
    "                            filepath, \n",
    "                            monitor='val_loss', \n",
    "                            verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            mode='min')\n",
    "\n",
    "patience = 500\n",
    "early_stopping = EarlyStopping(\n",
    "                            monitor='val_loss', \n",
    "                            min_delta=0, \n",
    "                            patience=patience, \n",
    "                            verbose=0, \n",
    "                            mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint_acc, checkpoint_loss, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **train_params)\n",
    "validation_generator = DataGenerator(partition['valid'], labels, **valid_params)\n",
    "testing_generator = DataGenerator(partition['test'], labels, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 'model_c3d_025'\n",
    "models_dir = join(\"..\", \"models\", model_iteration)\n",
    "model_name = join(models_dir, model_iteration + '.h5')\n",
    "history_name = join(models_dir, model_iteration + '_history.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads unique names for specific training session\n",
    "model_dir = config_clips.models_dir\n",
    "model_name = config_clips.model_name\n",
    "history_name = config_clips.history_name\n",
    "\n",
    "# makes new directory to place all saved files\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# loads a model\n",
    "model = load_model(dense_activation='relu')\n",
    "layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "for layer in model.layers:\n",
    "    if layer.name in layers_to_train:\n",
    "        layer.trainable = True\n",
    "        # print('{} IS trainable'.format(layer.name))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "# compiles a model \n",
    "adam = optimizers.adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "# trains a model\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=400,\n",
    "                    initial_epoch=0,\n",
    "                    workers=6)\n",
    "\n",
    "# Saves final model and training results\n",
    "# print('Saving model as {}'.format(model_name))\n",
    "model.save(model_name)\n",
    "pickle.dump(history.history, open(history_name, \"wb\" ))\n",
    "\n",
    "# Runs inference verbosely over datasets\n",
    "run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads unique names for specific training session\n",
    "model_dir = config_clips.models_dir\n",
    "model_name = config_clips.model_name\n",
    "history_name = config_clips.history_name\n",
    "\n",
    "# makes new directory to place all saved files\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# loads a model\n",
    "model = load_model(dense_activation='softmax')\n",
    "layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "for layer in model.layers:\n",
    "    if layer.name in layers_to_train:\n",
    "        layer.trainable = True\n",
    "        # print('{} IS trainable'.format(layer.name))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "# compiles a model \n",
    "adam = optimizers.adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "# trains a model\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=200,\n",
    "                    initial_epoch=0,\n",
    "                    workers=6)\n",
    "# model.save(model_name)\n",
    "weights_name = model_name[:-2] + 'hdf5'\n",
    "model.save_weights(weights_name)\n",
    "\n",
    "# Retrain model with new activation\n",
    "model = load_model(dense_activation='relu')\n",
    "model.load_weights(weights_name)\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=400,\n",
    "                    initial_epoch=200,\n",
    "                    workers=6)\n",
    "# Saves final model and training results\n",
    "# print('Saving model as {}'.format(model_name))\n",
    "model.save(model_name)\n",
    "pickle.dump(history.history, open(history_name, \"wb\" ))\n",
    "\n",
    "# Runs inference verbosely over datasets\n",
    "run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_trn = config_clips.train_data_dir\n",
    "PATH_val = config_clips.valid_data_dir\n",
    "PATH_test =  config_clips.test_data_dir\n",
    "PATH = config_clips.dataset_dir\n",
    "sizes = config_clips.sizes\n",
    "classes_to_nums = config_clips.classes_to_nums\n",
    "train_params = config_clips.train_params\n",
    "valid_params = config_clips.valid_params\n",
    "test_params = config_clips.test_params\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **train_params)\n",
    "validation_generator = DataGenerator(partition['valid'], labels, **valid_params)\n",
    "testing_generator = DataGenerator(partition['test'], labels, **test_params)\n",
    "\n",
    "model_iteration = 'model_c3d_025'\n",
    "models_dir = join(\"..\", \"models\", model_iteration)\n",
    "model_name = join(models_dir, model_iteration + '.h5')\n",
    "history_name = join(models_dir, model_iteration + '_history.pkl')\n",
    "\n",
    "\n",
    "\n",
    "filepath = join(model_dir, \"weights-acc-improvement-{epoch:03d}-{val_acc:.4f}.hdf5\")\n",
    "checkpoint_acc = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "filepath = join(model_dir, \"weights-loss-improvement-{epoch:03d}-{val_loss:.4f}.hdf5\")\n",
    "checkpoint_loss = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint_acc, checkpoint_loss]\n",
    "\n",
    "# loads unique names for specific training session\n",
    "model_dir = config_clips.models_dir\n",
    "model_name = config_clips.model_name\n",
    "history_name = config_clips.history_name\n",
    "\n",
    "# makes new directory to place all saved files\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# loads a model\n",
    "model = load_model(dense_activation='relu')\n",
    "layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "for layer in model.layers:\n",
    "    if layer.name in layers_to_train:\n",
    "        layer.trainable = True\n",
    "        # print('{} IS trainable'.format(layer.name))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "# compiles a model \n",
    "adam = optimizers.adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "# trains a model\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=400,\n",
    "                    initial_epoch=0,\n",
    "                    workers=6)\n",
    "\n",
    "# Saves final model and training results\n",
    "# print('Saving model as {}'.format(model_name))\n",
    "model.save(model_name)\n",
    "pickle.dump(history.history, open(history_name, \"wb\" ))\n",
    "\n",
    "# Runs inference verbosely over datasets\n",
    "run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)\n",
    "\n",
    "# loads unique names for specific training session\n",
    "model_dir = config_clips.models_dir\n",
    "model_name = config_clips.model_name\n",
    "history_name = config_clips.history_name\n",
    "\n",
    "# makes new directory to place all saved files\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# loads a model\n",
    "model = load_model(dense_activation='softmax')\n",
    "layers_to_train = ['fc7', 'fc8', 'fc9']\n",
    "for layer in model.layers:\n",
    "    if layer.name in layers_to_train:\n",
    "        layer.trainable = True\n",
    "        # print('{} IS trainable'.format(layer.name))\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        # print('{} is NOT trainable'.format(layer.name))\n",
    "\n",
    "# compiles a model \n",
    "adam = optimizers.adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "# trains a model\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=200,\n",
    "                    initial_epoch=0,\n",
    "                    workers=6)\n",
    "# model.save(model_name)\n",
    "weights_name = model_name[:-2] + 'hdf5'\n",
    "model.save_weights(weights_name)\n",
    "\n",
    "# Retrain model with new activation\n",
    "model = load_model(dense_activation='relu')\n",
    "model.load_weights(weights_name)\n",
    "history = model.fit_generator(\n",
    "                    generator=training_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=400,\n",
    "                    initial_epoch=200,\n",
    "                    workers=6)\n",
    "# Saves final model and training results\n",
    "# print('Saving model as {}'.format(model_name))\n",
    "model.save(model_name)\n",
    "pickle.dump(history.history, open(history_name, \"wb\" ))\n",
    "\n",
    "# Runs inference verbosely over datasets\n",
    "run_verbose_inference(model, model_dir, training_generator, validation_generator, testing_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
